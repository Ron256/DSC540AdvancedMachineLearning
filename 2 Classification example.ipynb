{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVniDlM8p1bo",
    "tags": []
   },
   "source": [
    "# Machine Learning\n",
    "### DePaul University\n",
    "Ilyas Ustun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVniDlM8p1bo",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# What is classification?\n",
    "\n",
    "###  Which of the following is a classification problem?\n",
    "Developing a good intuition of classification problems is important. Of the scenarios listed below select the ones that you think are classification problems and justify your choice.\n",
    "\n",
    " 1. Using labeled historic pricing data to predict if the price of gold will increase or decrease tomorrow. This is classification\n",
    " 2. Using labeled pricing data to predict the price of gold tomorrow. This is regression.\n",
    " 3. Using unlabeled data to cluster job candidates into roles. Not classification. \n",
    " 4. Using labeled data to predict the number of sales of a new song. This is Regression because of sales\n",
    " 5. Training a drone to recognize a certain type of terrain from labeled data. This is Classification\n",
    " 6. Using labeled flower data, predict the type of a new flower. This is classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of IRIS dataset using Scikit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-517dbfc3b547>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRIS data set\n",
    "#### https://en.wikipedia.org/wiki/Iris_flower_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg\", width=200, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Iris dataset from Scikit-Learn's datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print (\"Shape of the data \", iris.data.shape) # shape of the data\n",
    "print (\"Shape of the data \", iris.target_names)\n",
    "print (\"Attributes \", iris.feature_names)\n",
    "\n",
    "#view first 5 rows\n",
    "print (iris.data[range(5)])\n",
    "print (iris.target[range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show it as a table\n",
    "df = pd.DataFrame(data=iris.data) # converting the input data into a dataframe\n",
    "df.columns = iris.feature_names   # assigning column names\n",
    "df['Class'] = iris.target         # adding a new column to the data frame.\n",
    "df['Name'] = iris.target_names[iris.target] # adding a new column to the data frame\n",
    "df.head() # first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail() # the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='sepal length (cm)', y='sepal width (cm)', hue='Class');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='sepal length (cm)', y='sepal width (cm)', hue='Name');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>So using a distinct categorical vaariable produces a better pallet</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the X and y arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2]  # we only take the first two features all rows\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:6, :] # check the first 6 rows of the columns in our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataframe this can be done by using the .iloc() or .loc() methods. However the datasets returned will be a dataframe for X, and pandas series for y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me back all the rows but only these two columns\n",
    "X = df.loc[:, ['sepal length (cm)', 'sepal width (cm)']]  # we only take the first two features.\n",
    "y = df.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y # this is a pandas series, which is a one dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`sklearn` accepts both arrays and dataframes, so both methods are  OK.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# test_size of 0.33 defines that I want 30% to go into my testing\n",
    "# random_state with a number defines that you have the same result.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# instantiation\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "# fit the classifier on the data\n",
    "# model fitting\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# output the classifiers prediction on training set\n",
    "pred_train = logreg.predict(X_train)\n",
    "\n",
    "# output the classifiers prediction on testing set\n",
    "pred_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "# comparing the actual values, y train with the predicted values\n",
    "(y_train == pred_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test == pred_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In general for larger normal datasets, the training accuracy should be higher and the testing accuracy should be lower.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassification rate on the training\n",
    "(y_train != pred_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassification rate on the testing data set\n",
    "(y_test != pred_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Predicted'] = pred_test\n",
    "X_test['Actual'] = y_test\n",
    "# df['Predicted Name'] = iris.target_names[predicted]\n",
    "#df.head()\n",
    "X_test.tail() #end of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Here, we tire everything back to the testing dataset.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Name\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_labels = [\"setosa\", \"versicolor\", \"virginica\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "\n",
    "def plot_clf_boundary(X, y, legend_labels):\n",
    "    \n",
    "    xlabel = X.columns.tolist()[0]\n",
    "    ylabel = X.columns.tolist()[1]\n",
    "    \n",
    "    X = X.values.copy()\n",
    "    \n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1, )\n",
    "\n",
    "    fig_kw = {\"figsize\":(7, 5)}\n",
    "    fig, ax = plt.subplots(**fig_kw)\n",
    "\n",
    "    ax.pcolormesh(xx, yy, Z, cmap=plt.cm.rainbow, shading='auto')\n",
    "\n",
    "    # Plot also the training points\n",
    "    kwargs = {'edgecolor':\"k\",\n",
    "                 # 'facecolor':\"k\",\n",
    "                 'linewidth':1,\n",
    "                 'linestyle':'--',\n",
    "                }\n",
    "\n",
    "    # sns.scatterplot(data=X_test, x=\"sepal length (cm)\", y=\"sepal width (cm)\", hue=y_test, palette=plt.cm.rainbow, **kwargs)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.rainbow)\n",
    "    plt.xlabel(xlabel, fontdict = {'size': 16})\n",
    "    plt.ylabel(ylabel, fontdict = {'size': 16})\n",
    "\n",
    "\n",
    "    # produce a legend with the unique colors from the scatter\n",
    "    # get the handles and labels\n",
    "    handles, _ = scatter.legend_elements()\n",
    "    legend1 = ax.legend(# *scatter.legend_elements(),\n",
    "                        handles, legend_labels,\n",
    "                        loc=\"lower left\", title=\"Classes\")\n",
    "    ax.add_artist(legend1)\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clf_boundary(X=X_test, y=y_test, legend_labels=legend_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This means that there are three classes: setosa, versicolor, virginica in purple, greenish and red respectively. This means that if a point ended up in purple color, it will be predicted as setosa, if it ended up in the grenish area, it will be predicted as versicolor and so on. Everything in purple agrees with each other. In the greenish area, we have some red points and this is means that there is a mis-classification; the same that applies to the greenish points in the red area, there are on the wrong side of the boundary.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix =  pd.crosstab(index = y_test, columns=pred_test.ravel(), rownames=['Expected'], colnames=['Predicted'])\n",
    "sns.heatmap(confusion_matrix, annot=True, square=False, fmt='', cbar=False)\n",
    "plt.title(\"Classification Matrix\", fontsize = 15)\n",
    "plt.show() # this is one way to create one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The same plot is plotted as above though it looks better.Class 0 and 2 are predcited correctly.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_true=y_test, y_pred=pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:cornflowerblue\">Exercise:</span>\n",
    "\n",
    "1. Train two other classifiers\n",
    "2. Plot classification matrix\n",
    "3. Compare their performance with the Logistic Regression Classifier. \n",
    "4. Here is list of available classifiers  http://scikit-learn.org/stable/supervised_learning.html#supervised-learning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
